library(ggplot2)
library(DataComputing)
library(MASS)
library(lqmm)

# 2 dimension

n=1000
sigma<-matrix(c(2,0,0,2.1),byrow = T,nrow = 2)     #sigma for diagonal covariance matrix
matrix1<-mvrnorm(n,rep(0,2),sigma)

sigma2<-matrix(c(2,1.5,1.5,2.1),byrow = T,nrow = 2)  # add off-diagonal terms
matrix2<-mvrnorm(n,rep(0,2),sigma2)
matrix_x<-rbind(matrix1,matrix2)
Y<-as.factor(matrix(c(rep(0,1000),rep(1,1000)),byrow=F,nrow=2000))
matrix_2<-as.data.frame(cbind(matrix_x,Y))

set.seed(200)          # set the table and training & test set
table_2<-matrix_2[sample(nrow(matrix_2)),] %>% dplyr::rename( X1 = V1, X2 = V2,Y = V3)
table_2$Y<-as.factor(table_2$Y)
train<-table_2[1:1000,]
test<-table_2[1001:2000,]

  ##classification

   ###rf
library(randomForest)
fit.rf.2 <- randomForest(as.factor(Y) ~ X1+X2, data=train, ntree=100)
summary(fit.rf.2)
prediction.rf<-predict(fit.rf.2,test[,1:2])
View(fit.rf.2$confusion)
err.rate.rf <- sum(prediction.rf != test[,3]) / length(Y)
err.rate.rf


    ## bad separation
ggplot(data = table_2,aes(x=X1,y=X2))+geom_jitter(aes(color=Y))+labs(title="Two Dimensions Separation",x='X1',y='X2')
                                                      
    
                                  

#10 dimensions
diag<-c(2.1,2.3,2.2,2.22,2.21,2.31,2.27,2.19,2.28,2.34)  # create positive definite covariance matrix
sigma_10_2<-matrix(c(rep(0,100)),byrow = T,nrow = 10)
for(i in 1:10){
  sigma_10_2[i,i]<-diag[i]
}
for(i in 1:9){
  set.seed(100)
  for(j in (i+1):10){
    sigma_10_2[i,j]<-runif(1,0,10)
    sigma_10_2[j,i]<-sigma_10_2[i,j]
  }
}
sigma_10_2<-make.positive.definite(sigma_10_2)
matrix_10_2<-mvrnorm(n,rep(0,10),sigma_10_2)


sigma_10_1<-matrix(c(rep(0,100)),byrow = T,nrow = 10)  # create diagonal matrix
for(i in 1:10){
  sigma_10_1[i,i]<-sigma_10_2[i,i]
}
matrix_10_1<-mvrnorm(n,rep(0,10),sigma_10_1)

matrix_10_x<-rbind(matrix_10_1,matrix_10_2)   ##combine into a table
matrix_10_tt<-as.data.frame(cbind(matrix_10_x,Y))
set.seed(98)
table_10_total<-matrix_10_tt[sample(nrow(matrix_10_tt)),]%>%dplyr::rename(X1=V1,X2=V2,X3=V3,X4=V4,X5=V5,X6=V6,X7=V7,X8=V8,X9=V9,X10=V10,Y=V11)
table_10_total$Y<-as.factor(table_10_total$Y)
train_10<-table_10_total[1:1000,]
test_10<-table_10_total[1001:2000,]

  ##10D PCA
require(devtools)
install_github("ggbiplot", "vqv")
library(ggbiplot)

matrix.x <- table_10_total[, 1:10]
Y_10 <- table_10_total[, 11]

   # apply PCA, remember to scale and center to work with normalized data
tt10.pca <- prcomp(matrix.x, center = TRUE, scale. = TRUE)

   # print the rotation and stretch matrix
print(tt10.pca)

   # plot the variances along each PC
plot(tt10.pca, type = "l", main = "Principal Variances")

   # summary method
summary(tt10.pca)

   # Predict PCs of new observations
predict(tt10.pca, newdata=tail(matrix.x, 2))


   # Fancier Biplot
p <- ggbiplot(tt10.pca, obs.scale = 1, var.scale = 1, groups = Y_10, ellipse = TRUE, circle = TRUE)
p <- p + scale_color_discrete(name = '')
p <- p + theme(legend.direction = 'horizontal', legend.position = 'top')
p

    ##rf classification
fit.rf.10 <- randomForest(as.factor(train_10$Y) ~ ., data=train_10, ntree=100)
summary(fit.rf.10)
prediction.rf.10<-predict(fit.rf.10,test_10[,1:10])
err.rate.rf.10 <- sum(prediction.rf.10 != test_10$Y) / length(Y)
err.rate.rf.10



#3. add dimensions


 #pca separation
s=1000
diag_more<-c(runif(s,1.9,3.1))  # create positive definite covariance matrix
sigma_s_2<-matrix(c(rep(0,s*s)),byrow = T,nrow = s)
for(i in 1:s){
  sigma_s_2[i,i]<-diag_more[i]
}
for(i in 1:(s-1)){
  set.seed(100)
  for(j in (i+1):s){
    sigma_s_2[i,j]<-runif(1,0,10)
    sigma_s_2[j,i]<-sigma_s_2[i,j]
  }
}
sigma_s_2<-make.positive.definite(sigma_s_2)
matrix_s_2<-mvrnorm(n,rep(0,s),sigma_s_2)


sigma_s_1<-matrix(c(rep(0,s*s)),byrow = T,nrow = s)  # create diagonal matrix
for(i in 1:s){
  sigma_s_1[i,i]<-sigma_s_2[i,i]
}
matrix_s_1<-mvrnorm(n,rep(0,s),sigma_s_1)

matrix_s_x<-rbind(matrix_s_1,matrix_s_2)   ##combine into a table
matrix_s_tt<-as.data.frame(cbind(matrix_s_x,Y))

#FANCIER
more.pca <- prcomp(matrix_s_tt[,1:s], center = TRUE, scale. = TRUE)
p <- ggbiplot(more.pca, obs.scale = 1, var.scale = 1, groups = Y, ellipse = TRUE, circle = TRUE)
p <- p + scale_color_discrete(name = '')
p <- p + theme(legend.direction = 'horizontal', legend.position = 'top')
p







## pca class?
str(tt10.pca$rotation)
A<-tt10.pca$rotation
b<-as.matrix(matrix.x)%*%as.matrix(A)
t_b<-as.data.frame(cbind(b,as.data.frame(Y_10)))
train_b_10<-t_b[1:1000,]
test_b_10<-t_b[1001:2000,]


fit.rf.10.PC <- randomForest(as.factor(Y_10) ~ ., data=train_b_10, ntree=100)
prediction.rf.10.tb<-predict(fit.rf.10.PC,test_b_10[,1:10])
err.rate.rf.10.PC <- sum(prediction.rf.10.tb != test_b_10$Y) / length(Y)
err.rate.rf.10.PC
